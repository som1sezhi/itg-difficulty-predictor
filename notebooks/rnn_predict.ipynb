{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28a5cff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4867e097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "\n",
    "torch.set_default_device(device)\n",
    "print(f\"Using device = {torch.get_default_device()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2c60b07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def nps_seq_to_tensor(seq):\n",
    "    return torch.tensor(seq, dtype=torch.float32).reshape((-1, 1, 1))\n",
    "\n",
    "class ChartDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        self.data_tensors = list(map(\n",
    "            lambda seq: nps_seq_to_tensor(json.loads(seq)),\n",
    "            df['NPS sequence']\n",
    "        ))\n",
    "        self.meter_tensors = list(map(\n",
    "            lambda m: torch.tensor([[m]], dtype=torch.float32),\n",
    "            df['Meter']\n",
    "        ))\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_tensors)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.meter_tensors[i], self.data_tensors[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bec53851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1666 items\n",
      "example = (tensor([[11.]], device='cuda:0'), tensor([[[0.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[4.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[3.]],\n",
      "\n",
      "        [[6.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[5.]],\n",
      "\n",
      "        [[5.]],\n",
      "\n",
      "        [[5.]],\n",
      "\n",
      "        [[6.]],\n",
      "\n",
      "        [[5.]],\n",
      "\n",
      "        [[5.]],\n",
      "\n",
      "        [[6.]],\n",
      "\n",
      "        [[5.]],\n",
      "\n",
      "        [[6.]],\n",
      "\n",
      "        [[5.]],\n",
      "\n",
      "        [[4.]],\n",
      "\n",
      "        [[6.]],\n",
      "\n",
      "        [[5.]],\n",
      "\n",
      "        [[7.]],\n",
      "\n",
      "        [[6.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[3.]],\n",
      "\n",
      "        [[6.]],\n",
      "\n",
      "        [[6.]],\n",
      "\n",
      "        [[5.]],\n",
      "\n",
      "        [[6.]],\n",
      "\n",
      "        [[6.]],\n",
      "\n",
      "        [[6.]],\n",
      "\n",
      "        [[7.]],\n",
      "\n",
      "        [[6.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[6.]],\n",
      "\n",
      "        [[5.]],\n",
      "\n",
      "        [[5.]],\n",
      "\n",
      "        [[5.]],\n",
      "\n",
      "        [[6.]],\n",
      "\n",
      "        [[5.]],\n",
      "\n",
      "        [[5.]],\n",
      "\n",
      "        [[6.]],\n",
      "\n",
      "        [[5.]],\n",
      "\n",
      "        [[6.]],\n",
      "\n",
      "        [[5.]],\n",
      "\n",
      "        [[4.]],\n",
      "\n",
      "        [[6.]],\n",
      "\n",
      "        [[6.]],\n",
      "\n",
      "        [[7.]],\n",
      "\n",
      "        [[6.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[3.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[4.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[2.]],\n",
      "\n",
      "        [[6.]],\n",
      "\n",
      "        [[4.]],\n",
      "\n",
      "        [[7.]],\n",
      "\n",
      "        [[5.]],\n",
      "\n",
      "        [[5.]],\n",
      "\n",
      "        [[7.]],\n",
      "\n",
      "        [[6.]],\n",
      "\n",
      "        [[6.]],\n",
      "\n",
      "        [[5.]],\n",
      "\n",
      "        [[6.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[9.]],\n",
      "\n",
      "        [[8.]],\n",
      "\n",
      "        [[3.]]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "all_data = ChartDataset('rnn_dataset.csv')\n",
    "print(f'loaded {len(all_data)} items')\n",
    "print(f'example = {all_data[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "386ddbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train examples = 1333, validation examples = 333\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = torch.utils.data.random_split(all_data, [.8, .2], generator=torch.Generator(device=device).manual_seed(2024))\n",
    "\n",
    "print(f\"train examples = {len(train_set)}, validation examples = {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "98dea2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatingRNN(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(RatingRNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(1, hidden_size, nonlinearity='relu')\n",
    "        self.h2o = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, line_tensor):\n",
    "        rnn_out, hidden = self.rnn(line_tensor)\n",
    "        output = self.h2o(hidden[0])\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c52b8f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RatingRNN(\n",
      "  (rnn): RNN(1, 32)\n",
      "  (h2o): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "rnn = RatingRNN(32)\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ca16747b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[53.5026]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "bpm = 10000\n",
    "inp = nps_seq_to_tensor([bpm*4/60]*round(60/bpm/4*16*32))\n",
    "out = rnn(inp)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9b1d20c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def train(rnn, training_data, n_epoch=10, n_batch_size=64, report_every=50, learning_rate=0.2, criterion=nn.MSELoss()):\n",
    "    \"\"\"\n",
    "    Learn on a batch of training_data for a specified number of iterations and reporting thresholds\n",
    "    \"\"\"\n",
    "    # Keep track of losses for plotting\n",
    "    current_loss = 0\n",
    "    all_losses = []\n",
    "    rnn.train()\n",
    "    optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "    # start = time.time()\n",
    "    print(f\"training on data set with n = {len(training_data)}\")\n",
    "\n",
    "    for iter in range(1, n_epoch + 1):\n",
    "        rnn.zero_grad() # clear the gradients\n",
    "\n",
    "        # create some minibatches\n",
    "        # we cannot use dataloaders because each of our names is a different length\n",
    "        batches = list(range(len(training_data)))\n",
    "        random.shuffle(batches)\n",
    "        batches = np.array_split(batches, len(batches) // n_batch_size )\n",
    "\n",
    "        for idx, batch in enumerate(batches):\n",
    "            batch_loss = 0\n",
    "            for i in batch: #for each example in this batch\n",
    "                (meter_tensor, seq_tensor) = training_data[i]\n",
    "                output = rnn.forward(seq_tensor)\n",
    "                loss = criterion(output, meter_tensor)\n",
    "                batch_loss += loss\n",
    "\n",
    "            # optimize parameters\n",
    "            batch_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(rnn.parameters(), 3)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            current_loss += batch_loss.item() / len(batch)\n",
    "\n",
    "        all_losses.append(current_loss / len(batches) )\n",
    "        if iter % report_every == 0:\n",
    "            print(f\"{iter} ({iter / n_epoch:.0%}): \\t average batch loss = {all_losses[-1]}\")\n",
    "        current_loss = 0\n",
    "\n",
    "    return all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "057ab780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on data set with n = 1333\n",
      "1 (0%): \t average batch loss = 505.34300717647545\n",
      "2 (0%): \t average batch loss = 500.45008215315465\n",
      "3 (0%): \t average batch loss = 495.7884829006601\n",
      "4 (1%): \t average batch loss = 491.2391457573637\n",
      "5 (1%): \t average batch loss = 487.11284243731643\n",
      "6 (1%): \t average batch loss = 483.4933893211499\n",
      "7 (1%): \t average batch loss = 480.21049530843646\n",
      "8 (1%): \t average batch loss = 476.7673401679457\n",
      "9 (2%): \t average batch loss = 473.230740407515\n",
      "10 (2%): \t average batch loss = 469.73564313994655\n",
      "11 (2%): \t average batch loss = 466.223423498629\n",
      "12 (2%): \t average batch loss = 462.5757798145281\n",
      "13 (2%): \t average batch loss = 458.9663331955279\n",
      "14 (2%): \t average batch loss = 455.21549613880177\n",
      "15 (2%): \t average batch loss = 451.05214653826044\n",
      "16 (3%): \t average batch loss = 446.91307882639364\n",
      "17 (3%): \t average batch loss = 442.6135004355\n",
      "18 (3%): \t average batch loss = 438.0198258529793\n",
      "19 (3%): \t average batch loss = 433.2228593140476\n",
      "20 (3%): \t average batch loss = 428.27500594064475\n",
      "21 (4%): \t average batch loss = 423.0757171617057\n",
      "22 (4%): \t average batch loss = 417.629946127848\n",
      "23 (4%): \t average batch loss = 411.86862674818366\n",
      "24 (4%): \t average batch loss = 405.9472833585977\n",
      "25 (4%): \t average batch loss = 399.52210538218003\n",
      "26 (4%): \t average batch loss = 392.9152258946744\n",
      "27 (4%): \t average batch loss = 385.8443582866916\n",
      "28 (5%): \t average batch loss = 378.35681751929275\n",
      "29 (5%): \t average batch loss = 370.54731193552834\n",
      "30 (5%): \t average batch loss = 361.7120448229379\n",
      "31 (5%): \t average batch loss = 352.3602219946079\n",
      "32 (5%): \t average batch loss = 342.7521212077044\n",
      "33 (6%): \t average batch loss = 332.6022544857389\n",
      "34 (6%): \t average batch loss = 321.86956243286414\n",
      "35 (6%): \t average batch loss = 310.6488942781653\n",
      "36 (6%): \t average batch loss = 298.97757996240387\n",
      "37 (6%): \t average batch loss = 286.61772672945636\n",
      "38 (6%): \t average batch loss = 273.72316609645316\n",
      "39 (6%): \t average batch loss = 260.1696507475054\n",
      "40 (7%): \t average batch loss = 246.1710344098047\n",
      "41 (7%): \t average batch loss = 231.32185427399583\n",
      "42 (7%): \t average batch loss = 215.72355992365937\n",
      "43 (7%): \t average batch loss = 199.546438473029\n",
      "44 (7%): \t average batch loss = 182.75435354300936\n",
      "45 (8%): \t average batch loss = 165.51947878902715\n",
      "46 (8%): \t average batch loss = 147.59055002420428\n",
      "47 (8%): \t average batch loss = 129.7840776033716\n",
      "48 (8%): \t average batch loss = 111.8131254019321\n",
      "49 (8%): \t average batch loss = 94.38139263068534\n",
      "50 (8%): \t average batch loss = 78.54767933349058\n",
      "51 (8%): \t average batch loss = 66.62712318699911\n",
      "52 (9%): \t average batch loss = 60.197201956305115\n",
      "53 (9%): \t average batch loss = 58.315080024515645\n",
      "54 (9%): \t average batch loss = 56.92763265802114\n",
      "55 (9%): \t average batch loss = 55.462249054686616\n",
      "56 (9%): \t average batch loss = 53.82961458907522\n",
      "57 (10%): \t average batch loss = 52.11432056470136\n",
      "58 (10%): \t average batch loss = 50.06053059343729\n",
      "59 (10%): \t average batch loss = 48.44233478906735\n",
      "60 (10%): \t average batch loss = 47.083608157167816\n",
      "61 (10%): \t average batch loss = 45.72205725108699\n",
      "62 (10%): \t average batch loss = 44.485907962407424\n",
      "63 (10%): \t average batch loss = 43.2815979514603\n",
      "64 (11%): \t average batch loss = 42.31975832921464\n",
      "65 (11%): \t average batch loss = 41.294711763337965\n",
      "66 (11%): \t average batch loss = 40.11979289647952\n",
      "67 (11%): \t average batch loss = 39.05678304726589\n",
      "68 (11%): \t average batch loss = 37.79581831885052\n",
      "69 (12%): \t average batch loss = 36.66992267003052\n",
      "70 (12%): \t average batch loss = 35.62288595309143\n",
      "71 (12%): \t average batch loss = 34.54704151196698\n",
      "72 (12%): \t average batch loss = 33.291547468678\n",
      "73 (12%): \t average batch loss = 32.094027773452964\n",
      "74 (12%): \t average batch loss = 31.06031082546694\n",
      "75 (12%): \t average batch loss = 29.869579457007404\n",
      "76 (13%): \t average batch loss = 28.695714837897686\n",
      "77 (13%): \t average batch loss = 27.84965784287679\n",
      "78 (13%): \t average batch loss = 26.622606396340824\n",
      "79 (13%): \t average batch loss = 25.451901984937262\n",
      "80 (13%): \t average batch loss = 24.486080316330508\n",
      "81 (14%): \t average batch loss = 23.369104432477393\n",
      "82 (14%): \t average batch loss = 22.45608262813021\n",
      "83 (14%): \t average batch loss = 21.447781473519523\n",
      "84 (14%): \t average batch loss = 20.312671039224163\n",
      "85 (14%): \t average batch loss = 19.41415732464387\n",
      "86 (14%): \t average batch loss = 18.516886611310458\n",
      "87 (14%): \t average batch loss = 17.962659438817763\n",
      "88 (15%): \t average batch loss = 17.121288041796742\n",
      "89 (15%): \t average batch loss = 16.34678898280371\n",
      "90 (15%): \t average batch loss = 15.59832232202248\n",
      "91 (15%): \t average batch loss = 14.60604821476447\n",
      "92 (15%): \t average batch loss = 13.902022846485897\n",
      "93 (16%): \t average batch loss = 13.108190796505752\n",
      "94 (16%): \t average batch loss = 12.37885427073837\n",
      "95 (16%): \t average batch loss = 11.774739298999497\n",
      "96 (16%): \t average batch loss = 11.234397318480728\n",
      "97 (16%): \t average batch loss = 10.577281189309911\n",
      "98 (16%): \t average batch loss = 9.960540361201693\n",
      "99 (16%): \t average batch loss = 9.457641530824107\n",
      "100 (17%): \t average batch loss = 9.02750844692332\n",
      "101 (17%): \t average batch loss = 8.684248451774664\n",
      "102 (17%): \t average batch loss = 8.31108096125761\n",
      "103 (17%): \t average batch loss = 8.273538834991523\n",
      "104 (17%): \t average batch loss = 7.850737903237828\n",
      "105 (18%): \t average batch loss = 7.487067131411701\n",
      "106 (18%): \t average batch loss = 7.257774989327819\n",
      "107 (18%): \t average batch loss = 7.053754590335123\n",
      "108 (18%): \t average batch loss = 6.761816106043159\n",
      "109 (18%): \t average batch loss = 6.495758750221947\n",
      "110 (18%): \t average batch loss = 6.4170025618892605\n",
      "111 (18%): \t average batch loss = 6.150800374901063\n",
      "112 (19%): \t average batch loss = 6.096952735628278\n",
      "113 (19%): \t average batch loss = 5.967246732534946\n",
      "114 (19%): \t average batch loss = 5.908655786363316\n",
      "115 (19%): \t average batch loss = 5.749650315276534\n",
      "116 (19%): \t average batch loss = 5.721473575317715\n",
      "117 (20%): \t average batch loss = 5.634624357667781\n",
      "118 (20%): \t average batch loss = 5.5717745048974985\n",
      "119 (20%): \t average batch loss = 5.535013488879952\n",
      "120 (20%): \t average batch loss = 5.606388445732984\n",
      "121 (20%): \t average batch loss = 5.403349071280118\n",
      "122 (20%): \t average batch loss = 5.348886450358185\n",
      "123 (20%): \t average batch loss = 5.281934613418062\n",
      "124 (21%): \t average batch loss = 5.238926125349151\n",
      "125 (21%): \t average batch loss = 5.274174495370934\n",
      "126 (21%): \t average batch loss = 5.061559545136533\n",
      "127 (21%): \t average batch loss = 5.095527906517054\n",
      "128 (21%): \t average batch loss = 5.0907972636884935\n",
      "129 (22%): \t average batch loss = 4.979790695411271\n",
      "130 (22%): \t average batch loss = 4.8879090485557715\n",
      "131 (22%): \t average batch loss = 4.8019097088796965\n",
      "132 (22%): \t average batch loss = 4.825034193041529\n",
      "133 (22%): \t average batch loss = 4.772144506722756\n",
      "134 (22%): \t average batch loss = 4.87347028155178\n",
      "135 (22%): \t average batch loss = 4.675741825595479\n",
      "136 (23%): \t average batch loss = 4.699271423878146\n",
      "137 (23%): \t average batch loss = 4.660485869864434\n",
      "138 (23%): \t average batch loss = 4.558313339385227\n",
      "139 (23%): \t average batch loss = 4.630836511143512\n",
      "140 (23%): \t average batch loss = 4.407880760134211\n",
      "141 (24%): \t average batch loss = 4.407333742035616\n",
      "142 (24%): \t average batch loss = 4.403797605489508\n",
      "143 (24%): \t average batch loss = 4.334734631749312\n",
      "144 (24%): \t average batch loss = 4.291697062091664\n",
      "145 (24%): \t average batch loss = 4.197987722175147\n",
      "146 (24%): \t average batch loss = 4.1697063494354065\n",
      "147 (24%): \t average batch loss = 4.097935604024721\n",
      "148 (25%): \t average batch loss = 4.10272538449526\n",
      "149 (25%): \t average batch loss = 3.9426831414935704\n",
      "150 (25%): \t average batch loss = 3.9896725472148753\n",
      "151 (25%): \t average batch loss = 3.8336783824491043\n",
      "152 (25%): \t average batch loss = 3.6840544826140076\n",
      "153 (26%): \t average batch loss = 3.561041218357785\n",
      "154 (26%): \t average batch loss = 3.4698993125580815\n",
      "155 (26%): \t average batch loss = 3.3668646932780497\n",
      "156 (26%): \t average batch loss = 3.407950787865592\n",
      "157 (26%): \t average batch loss = 3.3507618683703293\n",
      "158 (26%): \t average batch loss = 3.347358636670606\n",
      "159 (26%): \t average batch loss = 3.3401898086044057\n",
      "160 (27%): \t average batch loss = 3.304424237061928\n",
      "161 (27%): \t average batch loss = 3.3099892963493964\n",
      "162 (27%): \t average batch loss = 3.253904149932379\n",
      "163 (27%): \t average batch loss = 3.215791127736727\n",
      "164 (27%): \t average batch loss = 3.1931787545192516\n",
      "165 (28%): \t average batch loss = 3.2946754583240585\n",
      "166 (28%): \t average batch loss = 3.228676019993803\n",
      "167 (28%): \t average batch loss = 3.191255219734292\n",
      "168 (28%): \t average batch loss = 3.165940342613714\n",
      "169 (28%): \t average batch loss = 3.143558454750782\n",
      "170 (28%): \t average batch loss = 3.117485601596841\n",
      "171 (28%): \t average batch loss = 3.156731181358224\n",
      "172 (29%): \t average batch loss = 3.1148296947988223\n",
      "173 (29%): \t average batch loss = 3.095978220055838\n",
      "174 (29%): \t average batch loss = 3.0068611674694816\n",
      "175 (29%): \t average batch loss = 3.0604709378522426\n",
      "176 (29%): \t average batch loss = 3.022605872359118\n",
      "177 (30%): \t average batch loss = 3.014426514769183\n",
      "178 (30%): \t average batch loss = 3.0132201168451953\n",
      "179 (30%): \t average batch loss = 2.96763922226801\n",
      "180 (30%): \t average batch loss = 2.9270400802563996\n",
      "181 (30%): \t average batch loss = 2.953865827494\n",
      "182 (30%): \t average batch loss = 3.010727380228927\n",
      "183 (30%): \t average batch loss = 2.976641376414271\n",
      "184 (31%): \t average batch loss = 2.9289341867482266\n",
      "185 (31%): \t average batch loss = 2.919108181957312\n",
      "186 (31%): \t average batch loss = 2.9892533651254447\n",
      "187 (31%): \t average batch loss = 2.906686710373194\n",
      "188 (31%): \t average batch loss = 2.863714451576347\n",
      "189 (32%): \t average batch loss = 2.8535288178829084\n",
      "190 (32%): \t average batch loss = 2.8652811805245455\n",
      "191 (32%): \t average batch loss = 2.8958267524198185\n",
      "192 (32%): \t average batch loss = 2.795633498752133\n",
      "193 (32%): \t average batch loss = 2.8374400255745864\n",
      "194 (32%): \t average batch loss = 2.8885147847831663\n",
      "195 (32%): \t average batch loss = 2.7380900279044673\n",
      "196 (33%): \t average batch loss = 2.7726631766689787\n",
      "197 (33%): \t average batch loss = 2.817198268626406\n",
      "198 (33%): \t average batch loss = 2.790708812918937\n",
      "199 (33%): \t average batch loss = 2.7635853971414894\n",
      "200 (33%): \t average batch loss = 2.6789472219362787\n",
      "201 (34%): \t average batch loss = 2.7512028413355916\n",
      "202 (34%): \t average batch loss = 2.694263869914897\n",
      "203 (34%): \t average batch loss = 2.7506003042951748\n",
      "204 (34%): \t average batch loss = 2.708278036268519\n",
      "205 (34%): \t average batch loss = 2.6891833860482746\n",
      "206 (34%): \t average batch loss = 2.6824458159779274\n",
      "207 (34%): \t average batch loss = 2.6767341697447873\n",
      "208 (35%): \t average batch loss = 2.701143538062252\n",
      "209 (35%): \t average batch loss = 2.645390578623219\n",
      "210 (35%): \t average batch loss = 2.580216734207685\n",
      "211 (35%): \t average batch loss = 2.6817104543188153\n",
      "212 (35%): \t average batch loss = 2.5812840042907914\n",
      "213 (36%): \t average batch loss = 2.5686946271933455\n",
      "214 (36%): \t average batch loss = 2.6020934604832835\n",
      "215 (36%): \t average batch loss = 2.568689126161794\n",
      "216 (36%): \t average batch loss = 2.5673770854506546\n",
      "217 (36%): \t average batch loss = 2.5222339922569823\n",
      "218 (36%): \t average batch loss = 2.5277688042387494\n",
      "219 (36%): \t average batch loss = 2.4941228370545736\n",
      "220 (37%): \t average batch loss = 2.5110907313512056\n",
      "221 (37%): \t average batch loss = 2.4578313132208005\n",
      "222 (37%): \t average batch loss = 2.5172235722673366\n",
      "223 (37%): \t average batch loss = 2.460480419471219\n",
      "224 (37%): \t average batch loss = 2.482248218282754\n",
      "225 (38%): \t average batch loss = 2.4687500076777034\n",
      "226 (38%): \t average batch loss = 2.502306594572581\n",
      "227 (38%): \t average batch loss = 2.4390528278186925\n",
      "228 (38%): \t average batch loss = 2.416565081330243\n",
      "229 (38%): \t average batch loss = 2.4167388646001995\n",
      "230 (38%): \t average batch loss = 2.465832314907507\n",
      "231 (38%): \t average batch loss = 2.3882060562092375\n",
      "232 (39%): \t average batch loss = 2.448208206501119\n",
      "233 (39%): \t average batch loss = 2.384509666922512\n",
      "234 (39%): \t average batch loss = 2.423856468666522\n",
      "235 (39%): \t average batch loss = 2.3621448204561575\n",
      "236 (39%): \t average batch loss = 2.3449470533822145\n",
      "237 (40%): \t average batch loss = 2.3525737595094487\n",
      "238 (40%): \t average batch loss = 2.3839210841516625\n",
      "239 (40%): \t average batch loss = 2.3999582568427007\n",
      "240 (40%): \t average batch loss = 2.3674830741658033\n",
      "241 (40%): \t average batch loss = 2.317654505384179\n",
      "242 (40%): \t average batch loss = 2.3394502397362102\n",
      "243 (40%): \t average batch loss = 2.344497298331672\n",
      "244 (41%): \t average batch loss = 2.329338538706006\n",
      "245 (41%): \t average batch loss = 2.317617234791204\n",
      "246 (41%): \t average batch loss = 2.314992235236338\n",
      "247 (41%): \t average batch loss = 2.3129881962776615\n",
      "248 (41%): \t average batch loss = 2.3128408404401615\n",
      "249 (42%): \t average batch loss = 2.3105631161900684\n",
      "250 (42%): \t average batch loss = 2.2669887013049324\n",
      "251 (42%): \t average batch loss = 2.2796284233483184\n",
      "252 (42%): \t average batch loss = 2.3088210798183324\n",
      "253 (42%): \t average batch loss = 2.287681464338022\n",
      "254 (42%): \t average batch loss = 2.3002333177370704\n",
      "255 (42%): \t average batch loss = 2.2542598298209544\n",
      "256 (43%): \t average batch loss = 2.2629669292110064\n",
      "257 (43%): \t average batch loss = 2.197306067433523\n",
      "258 (43%): \t average batch loss = 2.3188164809773486\n",
      "259 (43%): \t average batch loss = 2.243665774382924\n",
      "260 (43%): \t average batch loss = 2.193488117236996\n",
      "261 (44%): \t average batch loss = 2.1908463119651764\n",
      "262 (44%): \t average batch loss = 2.2353515065131493\n",
      "263 (44%): \t average batch loss = 2.1641277451258554\n",
      "264 (44%): \t average batch loss = 2.2226402852935307\n",
      "265 (44%): \t average batch loss = 2.1710432949782175\n",
      "266 (44%): \t average batch loss = 2.239489412760638\n",
      "267 (44%): \t average batch loss = 2.2282535816090836\n",
      "268 (45%): \t average batch loss = 2.1513427699868464\n",
      "269 (45%): \t average batch loss = 2.185719370399865\n",
      "270 (45%): \t average batch loss = 2.203800470566976\n",
      "271 (45%): \t average batch loss = 2.1491591072686385\n",
      "272 (45%): \t average batch loss = 2.149985492504436\n",
      "273 (46%): \t average batch loss = 2.173844306407499\n",
      "274 (46%): \t average batch loss = 2.149545822419607\n",
      "275 (46%): \t average batch loss = 2.1441931937194845\n",
      "276 (46%): \t average batch loss = 2.1800868275477194\n",
      "277 (46%): \t average batch loss = 2.114898500567591\n",
      "278 (46%): \t average batch loss = 2.1019294477382537\n",
      "279 (46%): \t average batch loss = 2.1097417842635524\n",
      "280 (47%): \t average batch loss = 2.0900012535188384\n",
      "281 (47%): \t average batch loss = 2.113199519291553\n",
      "282 (47%): \t average batch loss = 2.1170555421854242\n",
      "283 (47%): \t average batch loss = 2.099751691503365\n",
      "284 (47%): \t average batch loss = 2.1347797982135224\n",
      "285 (48%): \t average batch loss = 2.0769896605175466\n",
      "286 (48%): \t average batch loss = 2.1270930245795\n",
      "287 (48%): \t average batch loss = 2.088051630673178\n",
      "288 (48%): \t average batch loss = 2.100855884957454\n",
      "289 (48%): \t average batch loss = 2.1028590241582727\n",
      "290 (48%): \t average batch loss = 2.0429179768279795\n",
      "291 (48%): \t average batch loss = 2.04624763628521\n",
      "292 (49%): \t average batch loss = 2.042640073802988\n",
      "293 (49%): \t average batch loss = 2.069681043637818\n",
      "294 (49%): \t average batch loss = 2.077649051471298\n",
      "295 (49%): \t average batch loss = 2.1060394587316216\n",
      "296 (49%): \t average batch loss = 2.154192973067457\n",
      "297 (50%): \t average batch loss = 2.1103118303834667\n",
      "298 (50%): \t average batch loss = 2.062037640144145\n",
      "299 (50%): \t average batch loss = 2.0216732014794525\n",
      "300 (50%): \t average batch loss = 2.058315979825162\n",
      "301 (50%): \t average batch loss = 2.069848739351422\n",
      "302 (50%): \t average batch loss = 2.0210392080244035\n",
      "303 (50%): \t average batch loss = 2.0306338732515403\n",
      "304 (51%): \t average batch loss = 2.021785167847646\n",
      "305 (51%): \t average batch loss = 2.0326585196844875\n",
      "306 (51%): \t average batch loss = 2.1200271217384143\n",
      "307 (51%): \t average batch loss = 2.031863516674275\n",
      "308 (51%): \t average batch loss = 2.018861977237751\n",
      "309 (52%): \t average batch loss = 2.005094026473247\n",
      "310 (52%): \t average batch loss = 2.0966238594659043\n",
      "311 (52%): \t average batch loss = 2.026189374902139\n",
      "312 (52%): \t average batch loss = 1.9784785832716083\n",
      "313 (52%): \t average batch loss = 2.014572123454632\n",
      "314 (52%): \t average batch loss = 1.9657895252533166\n",
      "315 (52%): \t average batch loss = 2.005254152402356\n",
      "316 (53%): \t average batch loss = 1.9821412243060208\n",
      "317 (53%): \t average batch loss = 1.9687662117732483\n",
      "318 (53%): \t average batch loss = 1.9679146767956595\n",
      "319 (53%): \t average batch loss = 1.9854503783420543\n",
      "320 (53%): \t average batch loss = 2.01675558594868\n",
      "321 (54%): \t average batch loss = 2.117962004662854\n",
      "322 (54%): \t average batch loss = 2.039031806266022\n",
      "323 (54%): \t average batch loss = 1.9832233895655946\n",
      "324 (54%): \t average batch loss = 1.9637938729350466\n",
      "325 (54%): \t average batch loss = 1.9550906234389243\n",
      "326 (54%): \t average batch loss = 1.9479250979067675\n",
      "327 (55%): \t average batch loss = 1.963625307311736\n",
      "328 (55%): \t average batch loss = 2.0093511125934658\n",
      "329 (55%): \t average batch loss = 1.9843227683230797\n",
      "330 (55%): \t average batch loss = 1.962154365998488\n",
      "331 (55%): \t average batch loss = 2.015191356481227\n",
      "332 (55%): \t average batch loss = 2.0493394606688184\n",
      "333 (56%): \t average batch loss = 1.979864650848817\n",
      "334 (56%): \t average batch loss = 1.9484830663949322\n",
      "335 (56%): \t average batch loss = 1.9200465910146807\n",
      "336 (56%): \t average batch loss = 1.9528962141784914\n",
      "337 (56%): \t average batch loss = 1.954486903600809\n",
      "338 (56%): \t average batch loss = 1.9546923891444485\n",
      "339 (56%): \t average batch loss = 1.9605883051830415\n",
      "340 (57%): \t average batch loss = 1.9493229894064392\n",
      "341 (57%): \t average batch loss = 1.960670191515953\n",
      "342 (57%): \t average batch loss = 1.9465300114431947\n",
      "343 (57%): \t average batch loss = 1.9140853754161768\n",
      "344 (57%): \t average batch loss = 1.9397419725484515\n",
      "345 (57%): \t average batch loss = 1.9272832418447816\n",
      "346 (58%): \t average batch loss = 1.9171694799981363\n",
      "347 (58%): \t average batch loss = 1.8904056082371405\n",
      "348 (58%): \t average batch loss = 1.9521507603072947\n",
      "349 (58%): \t average batch loss = 1.9036426933250616\n",
      "350 (58%): \t average batch loss = 1.9504683455316694\n",
      "351 (58%): \t average batch loss = 1.8960137905119123\n",
      "352 (59%): \t average batch loss = 1.9031541078137035\n",
      "353 (59%): \t average batch loss = 1.8751303901827643\n",
      "354 (59%): \t average batch loss = 1.9112675392050806\n",
      "355 (59%): \t average batch loss = 1.9315205010285155\n",
      "356 (59%): \t average batch loss = 1.9408204211955797\n",
      "357 (60%): \t average batch loss = 1.9487004196843327\n",
      "358 (60%): \t average batch loss = 1.8926083832615266\n",
      "359 (60%): \t average batch loss = 1.8738960681917007\n",
      "360 (60%): \t average batch loss = 1.950937733419445\n",
      "361 (60%): \t average batch loss = 1.9399923152052299\n",
      "362 (60%): \t average batch loss = 1.8729969892497522\n",
      "363 (60%): \t average batch loss = 1.844690259211422\n",
      "364 (61%): \t average batch loss = 1.869038498773665\n",
      "365 (61%): \t average batch loss = 1.9152040702409199\n",
      "366 (61%): \t average batch loss = 1.8658843171159805\n",
      "367 (61%): \t average batch loss = 1.883253941822785\n",
      "368 (61%): \t average batch loss = 1.8674226999822001\n",
      "369 (62%): \t average batch loss = 1.8302517680008994\n",
      "370 (62%): \t average batch loss = 1.7077839453260268\n",
      "371 (62%): \t average batch loss = 1.6441263398990436\n",
      "372 (62%): \t average batch loss = 1.653026701152082\n",
      "373 (62%): \t average batch loss = 1.753071104935638\n",
      "374 (62%): \t average batch loss = 1.6596201908313435\n",
      "375 (62%): \t average batch loss = 1.6658496006103403\n",
      "376 (63%): \t average batch loss = 1.6390313376672985\n",
      "377 (63%): \t average batch loss = 1.6284520962291151\n",
      "378 (63%): \t average batch loss = 1.6546329310230405\n",
      "379 (63%): \t average batch loss = 1.6327151022039352\n",
      "380 (63%): \t average batch loss = 1.6122295126878317\n",
      "381 (64%): \t average batch loss = 1.638707174924853\n",
      "382 (64%): \t average batch loss = 1.6187255838671082\n",
      "383 (64%): \t average batch loss = 1.6151396197142613\n",
      "384 (64%): \t average batch loss = 1.6283681756497939\n",
      "385 (64%): \t average batch loss = 1.7209406910089282\n",
      "386 (64%): \t average batch loss = 1.6425258626122647\n",
      "387 (64%): \t average batch loss = 1.622372017786655\n",
      "388 (65%): \t average batch loss = 1.6309656106093953\n",
      "389 (65%): \t average batch loss = 1.6155046693343806\n",
      "390 (65%): \t average batch loss = 1.6559355401496767\n",
      "391 (65%): \t average batch loss = 1.6287783580039865\n",
      "392 (65%): \t average batch loss = 1.641431424413963\n",
      "393 (66%): \t average batch loss = 1.6439748836501806\n",
      "394 (66%): \t average batch loss = 1.5776838688222818\n",
      "395 (66%): \t average batch loss = 1.5802504850479877\n",
      "396 (66%): \t average batch loss = 1.6111560255626052\n",
      "397 (66%): \t average batch loss = 1.6358371272231593\n",
      "398 (66%): \t average batch loss = 1.575238018734646\n",
      "399 (66%): \t average batch loss = 1.7624296190942481\n",
      "400 (67%): \t average batch loss = 1.655116949422173\n",
      "401 (67%): \t average batch loss = 1.6188758841471025\n",
      "402 (67%): \t average batch loss = 1.6320544221723636\n",
      "403 (67%): \t average batch loss = 1.646240802180007\n",
      "404 (67%): \t average batch loss = 1.63547423089699\n",
      "405 (68%): \t average batch loss = 1.5820354753250125\n",
      "406 (68%): \t average batch loss = 1.6030089808394608\n",
      "407 (68%): \t average batch loss = 1.609433208381035\n",
      "408 (68%): \t average batch loss = 1.643931038182335\n",
      "409 (68%): \t average batch loss = 1.545885921376477\n",
      "410 (68%): \t average batch loss = 1.6236082436325645\n",
      "411 (68%): \t average batch loss = 1.5886188925471794\n",
      "412 (69%): \t average batch loss = 1.5871329926517097\n",
      "413 (69%): \t average batch loss = 1.6156062869344132\n",
      "414 (69%): \t average batch loss = 1.5655235678725845\n",
      "415 (69%): \t average batch loss = 1.6518974301611231\n",
      "416 (69%): \t average batch loss = 1.6136173702914594\n",
      "417 (70%): \t average batch loss = 1.580312628467824\n",
      "418 (70%): \t average batch loss = 1.5986776230293178\n",
      "419 (70%): \t average batch loss = 1.6131195491927077\n",
      "420 (70%): \t average batch loss = 1.6002229718156982\n",
      "421 (70%): \t average batch loss = 1.6497782328073822\n",
      "422 (70%): \t average batch loss = 1.5912483791591139\n",
      "423 (70%): \t average batch loss = 1.6148330661733996\n",
      "424 (71%): \t average batch loss = 1.656850258401485\n",
      "425 (71%): \t average batch loss = 1.5933417023926606\n",
      "426 (71%): \t average batch loss = 1.613730171631494\n",
      "427 (71%): \t average batch loss = 1.6760643535909048\n",
      "428 (71%): \t average batch loss = 1.6237191800238695\n",
      "429 (72%): \t average batch loss = 1.6526681397091694\n",
      "430 (72%): \t average batch loss = 1.5905527028601827\n",
      "431 (72%): \t average batch loss = 1.6200436818551176\n",
      "432 (72%): \t average batch loss = 1.5927182240272635\n",
      "433 (72%): \t average batch loss = 1.647508439021324\n",
      "434 (72%): \t average batch loss = 1.629713984454503\n",
      "435 (72%): \t average batch loss = 1.6904696348079025\n",
      "436 (73%): \t average batch loss = 1.6151849377484193\n",
      "437 (73%): \t average batch loss = 1.616819605143671\n",
      "438 (73%): \t average batch loss = 1.5886225130589287\n",
      "439 (73%): \t average batch loss = 1.5394270478951233\n",
      "440 (73%): \t average batch loss = 1.6083307289527267\n",
      "441 (74%): \t average batch loss = 1.5843075821271368\n",
      "442 (74%): \t average batch loss = 1.581998474228064\n",
      "443 (74%): \t average batch loss = 1.587368330401244\n",
      "444 (74%): \t average batch loss = 1.5992146624423218\n",
      "445 (74%): \t average batch loss = 1.599208966707655\n",
      "446 (74%): \t average batch loss = 1.6213950050626604\n",
      "447 (74%): \t average batch loss = 1.6562566124869929\n",
      "448 (75%): \t average batch loss = 1.6328373655804895\n",
      "449 (75%): \t average batch loss = 1.5851369205614607\n",
      "450 (75%): \t average batch loss = 1.7027229648108722\n",
      "451 (75%): \t average batch loss = 1.5445493692512806\n",
      "452 (75%): \t average batch loss = 1.5680853782008535\n",
      "453 (76%): \t average batch loss = 1.53167670589397\n",
      "454 (76%): \t average batch loss = 1.5759720790661171\n",
      "455 (76%): \t average batch loss = 1.5582776934461107\n",
      "456 (76%): \t average batch loss = 1.5668432102867622\n",
      "457 (76%): \t average batch loss = 1.6327892770598997\n",
      "458 (76%): \t average batch loss = 1.5932577527415854\n",
      "459 (76%): \t average batch loss = 1.6195951215070281\n",
      "460 (77%): \t average batch loss = 1.562179899797996\n",
      "461 (77%): \t average batch loss = 1.5815776608422676\n",
      "462 (77%): \t average batch loss = 1.5622344114941096\n",
      "463 (77%): \t average batch loss = 1.5489598989810192\n",
      "464 (77%): \t average batch loss = 1.5923757627289632\n",
      "465 (78%): \t average batch loss = 1.595944021061932\n",
      "466 (78%): \t average batch loss = 1.52499211577472\n",
      "467 (78%): \t average batch loss = 1.693069068515317\n",
      "468 (78%): \t average batch loss = 1.5746281957259711\n",
      "469 (78%): \t average batch loss = 1.6035148610253507\n",
      "470 (78%): \t average batch loss = 1.575223393916868\n",
      "471 (78%): \t average batch loss = 1.5612913393963521\n",
      "472 (79%): \t average batch loss = 1.6138796629920797\n",
      "473 (79%): \t average batch loss = 1.6181836998230836\n",
      "474 (79%): \t average batch loss = 1.6013307417857061\n",
      "475 (79%): \t average batch loss = 1.5536859038726931\n",
      "476 (79%): \t average batch loss = 1.5820311300897965\n",
      "477 (80%): \t average batch loss = 1.6596208933843635\n",
      "478 (80%): \t average batch loss = 1.5888684079960858\n",
      "479 (80%): \t average batch loss = 1.5598734191832417\n",
      "480 (80%): \t average batch loss = 1.6227429163504485\n",
      "481 (80%): \t average batch loss = 1.6079465375233428\n",
      "482 (80%): \t average batch loss = 1.5934004798726984\n",
      "483 (80%): \t average batch loss = 1.5955577508727115\n",
      "484 (81%): \t average batch loss = 1.5744424363597385\n",
      "485 (81%): \t average batch loss = 1.5610204996862114\n",
      "486 (81%): \t average batch loss = 1.5701381986144438\n",
      "487 (81%): \t average batch loss = 1.5452961602075128\n",
      "488 (81%): \t average batch loss = 1.541404036319617\n",
      "489 (82%): \t average batch loss = 1.6305861900533176\n",
      "490 (82%): \t average batch loss = 1.591956013222724\n",
      "491 (82%): \t average batch loss = 1.5446812574057924\n",
      "492 (82%): \t average batch loss = 1.598021731641917\n",
      "493 (82%): \t average batch loss = 1.5724772724615723\n",
      "494 (82%): \t average batch loss = 1.5891032334098658\n",
      "495 (82%): \t average batch loss = 1.5570606974442582\n",
      "496 (83%): \t average batch loss = 1.6622134939786375\n",
      "497 (83%): \t average batch loss = 1.6284995568601972\n",
      "498 (83%): \t average batch loss = 1.591094753743296\n",
      "499 (83%): \t average batch loss = 1.6028331403762497\n",
      "500 (83%): \t average batch loss = 1.6480886739284837\n",
      "501 (84%): \t average batch loss = 1.5862813747721745\n",
      "502 (84%): \t average batch loss = 1.5934647863345792\n",
      "503 (84%): \t average batch loss = 1.6354535528568594\n",
      "504 (84%): \t average batch loss = 1.6301907795147355\n",
      "505 (84%): \t average batch loss = 1.5637511840399336\n",
      "506 (84%): \t average batch loss = 1.5433567060059088\n",
      "507 (84%): \t average batch loss = 1.6176501129181449\n",
      "508 (85%): \t average batch loss = 1.5864150055497548\n",
      "509 (85%): \t average batch loss = 1.546961609457802\n",
      "510 (85%): \t average batch loss = 1.5743919095643664\n",
      "511 (85%): \t average batch loss = 1.5642638351840705\n",
      "512 (85%): \t average batch loss = 1.6373184509484382\n",
      "513 (86%): \t average batch loss = 1.572875118385015\n",
      "514 (86%): \t average batch loss = 1.538608685687137\n",
      "515 (86%): \t average batch loss = 1.5699247188559489\n",
      "516 (86%): \t average batch loss = 1.554239000901007\n",
      "517 (86%): \t average batch loss = 1.6358049084729387\n",
      "518 (86%): \t average batch loss = 1.6053521136901108\n",
      "519 (86%): \t average batch loss = 1.5925897438673884\n",
      "520 (87%): \t average batch loss = 1.5702722373455158\n",
      "521 (87%): \t average batch loss = 1.5572910500349149\n",
      "522 (87%): \t average batch loss = 1.5645130912301122\n",
      "523 (87%): \t average batch loss = 1.5978043130489024\n",
      "524 (87%): \t average batch loss = 1.595212950549046\n",
      "525 (88%): \t average batch loss = 1.6234536983157004\n",
      "526 (88%): \t average batch loss = 1.55435085857362\n",
      "527 (88%): \t average batch loss = 1.5521238881287558\n",
      "528 (88%): \t average batch loss = 1.5561518407310095\n",
      "529 (88%): \t average batch loss = 1.615235908297811\n",
      "530 (88%): \t average batch loss = 1.5695878219086894\n",
      "531 (88%): \t average batch loss = 1.5676260163588849\n",
      "532 (89%): \t average batch loss = 1.5202672541707631\n",
      "533 (89%): \t average batch loss = 1.5951587459610792\n",
      "534 (89%): \t average batch loss = 1.5517669855443024\n",
      "535 (89%): \t average batch loss = 1.541544505084386\n",
      "536 (89%): \t average batch loss = 1.533958563164099\n",
      "537 (90%): \t average batch loss = 1.565784048117539\n",
      "538 (90%): \t average batch loss = 1.5953410174069176\n",
      "539 (90%): \t average batch loss = 1.598122821637054\n",
      "540 (90%): \t average batch loss = 1.5749935560774233\n",
      "541 (90%): \t average batch loss = 1.5870669987340797\n",
      "542 (90%): \t average batch loss = 1.591489800151685\n",
      "543 (90%): \t average batch loss = 1.5937201750542662\n",
      "544 (91%): \t average batch loss = 1.5350126836268627\n",
      "545 (91%): \t average batch loss = 1.6032314998431747\n",
      "546 (91%): \t average batch loss = 1.597118293920937\n",
      "547 (91%): \t average batch loss = 1.5815096859908222\n",
      "548 (91%): \t average batch loss = 1.5852711755622297\n",
      "549 (92%): \t average batch loss = 1.5382319066924564\n",
      "550 (92%): \t average batch loss = 1.5150550668889826\n",
      "551 (92%): \t average batch loss = 1.5528013199176898\n",
      "552 (92%): \t average batch loss = 1.582103523330309\n",
      "553 (92%): \t average batch loss = 1.5811395320780186\n",
      "554 (92%): \t average batch loss = 1.5627608610245503\n",
      "555 (92%): \t average batch loss = 1.58629090064578\n",
      "556 (93%): \t average batch loss = 1.530708690921088\n",
      "557 (93%): \t average batch loss = 1.5885788080326735\n",
      "558 (93%): \t average batch loss = 1.5677986711789342\n",
      "559 (93%): \t average batch loss = 1.600900720849074\n",
      "560 (93%): \t average batch loss = 1.5307837641977822\n",
      "561 (94%): \t average batch loss = 1.5870149896242998\n",
      "562 (94%): \t average batch loss = 1.57083102484625\n",
      "563 (94%): \t average batch loss = 1.6162829519450423\n",
      "564 (94%): \t average batch loss = 1.5497475232003988\n",
      "565 (94%): \t average batch loss = 1.575937998774687\n",
      "566 (94%): \t average batch loss = 1.5857814456692978\n",
      "567 (94%): \t average batch loss = 1.5890981471468453\n",
      "568 (95%): \t average batch loss = 1.6652086910970716\n",
      "569 (95%): \t average batch loss = 1.6484063935678825\n",
      "570 (95%): \t average batch loss = 1.7167069866882627\n",
      "571 (95%): \t average batch loss = 1.5900023269308254\n",
      "572 (95%): \t average batch loss = 1.6957630842425389\n",
      "573 (96%): \t average batch loss = 1.7095032156672965\n",
      "574 (96%): \t average batch loss = 1.6572849957773663\n",
      "575 (96%): \t average batch loss = 1.6963282899584833\n",
      "576 (96%): \t average batch loss = 1.7577695355702183\n",
      "577 (96%): \t average batch loss = 1.591644190146016\n",
      "578 (96%): \t average batch loss = 1.59758751531469\n",
      "579 (96%): \t average batch loss = 1.5813588211839418\n",
      "580 (97%): \t average batch loss = 1.6054528775985077\n",
      "581 (97%): \t average batch loss = 1.612293162749181\n",
      "582 (97%): \t average batch loss = 1.5269375013906132\n",
      "583 (97%): \t average batch loss = 1.6228361762092967\n",
      "584 (97%): \t average batch loss = 1.5720863916818937\n",
      "585 (98%): \t average batch loss = 1.5933749311491143\n",
      "586 (98%): \t average batch loss = 1.6973581623783054\n",
      "587 (98%): \t average batch loss = 1.655852651056905\n",
      "588 (98%): \t average batch loss = 1.6103378847140308\n",
      "589 (98%): \t average batch loss = 1.6820932122605357\n",
      "590 (98%): \t average batch loss = 1.592666993902475\n",
      "591 (98%): \t average batch loss = 1.6074303573960367\n",
      "592 (99%): \t average batch loss = 1.619436102806309\n",
      "593 (99%): \t average batch loss = 1.621450977679131\n",
      "594 (99%): \t average batch loss = 1.6081752692558602\n",
      "595 (99%): \t average batch loss = 1.6087296313799542\n",
      "596 (99%): \t average batch loss = 1.5836358870493779\n",
      "597 (100%): \t average batch loss = 1.6143882164853776\n",
      "598 (100%): \t average batch loss = 1.5901546442902288\n",
      "599 (100%): \t average batch loss = 1.5460722386270882\n",
      "600 (100%): \t average batch loss = 1.5999777760671736\n",
      "training took 9946.096268415451s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "all_losses = train(rnn, train_set, n_epoch=600, learning_rate=3e-4, report_every=1)\n",
    "end = time.time()\n",
    "print(f\"training took {end-start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "925e5b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34534534534534533\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate(rnn, testing_data):\n",
    "    width = 43-11+3\n",
    "    confusion = torch.zeros(width, width)\n",
    "\n",
    "    rnn.eval() #set to eval mode\n",
    "    with torch.no_grad(): # do not record the gradients during eval phase\n",
    "        count = 0\n",
    "        for i in range(len(testing_data)):\n",
    "            (meter_tensor, seq_tensor) = testing_data[i]\n",
    "            guess = rnn(seq_tensor)\n",
    "            true_meter = int(meter_tensor.item())\n",
    "            guess_meter = round(guess.item())\n",
    "            guess_meter = max(10, min(44, guess_meter))\n",
    "            if true_meter == guess_meter:\n",
    "                count += 1\n",
    "            confusion[true_meter - 10][guess_meter - 10] += 1\n",
    "        print(count / len(testing_data))\n",
    "\n",
    "    # Normalize by dividing every row by its sum\n",
    "    for i in range(width):\n",
    "        denom = confusion[i].sum()\n",
    "        if denom > 0:\n",
    "            confusion[i] = confusion[i] / denom\n",
    "\n",
    "    # Set up plot\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(confusion.cpu().numpy()) #numpy uses cpu here so we need to use a cpu version\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticks(np.arange(width))\n",
    "    ax.set_yticks(np.arange(width))\n",
    "\n",
    "    # Force label at every tick\n",
    "    # ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    # ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    # sphinx_gallery_thumbnail_number = 2\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "evaluate(rnn, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ff2810ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rnn.state_dict(), 'cp_32hidden_600epochs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7b9c42e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Parameter containing:\n",
      "tensor([[-0.0625],\n",
      "        [ 0.1647],\n",
      "        [-0.0424],\n",
      "        [-0.0252],\n",
      "        [-0.1291],\n",
      "        [-0.2163],\n",
      "        [-0.1198],\n",
      "        [ 0.0428],\n",
      "        [-0.0169],\n",
      "        [-0.1813],\n",
      "        [-0.0192],\n",
      "        [ 0.0585],\n",
      "        [ 0.0841],\n",
      "        [ 0.1422],\n",
      "        [ 0.0791],\n",
      "        [-0.1569],\n",
      "        [-0.0407],\n",
      "        [-0.1301],\n",
      "        [ 0.1153],\n",
      "        [-0.0475],\n",
      "        [-0.2394],\n",
      "        [ 0.0349],\n",
      "        [ 0.1424],\n",
      "        [ 0.1089],\n",
      "        [-0.1378],\n",
      "        [ 0.1503],\n",
      "        [-0.0650],\n",
      "        [-0.1713],\n",
      "        [-0.0723],\n",
      "        [ 0.0421],\n",
      "        [ 0.1052],\n",
      "        [ 0.1942]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0372, -0.1474,  0.0916,  ...,  0.0340,  0.1183,  0.1203],\n",
      "        [-0.0886, -0.0579,  0.0890,  ...,  0.0412,  0.0253, -0.0637],\n",
      "        [ 0.0885, -0.1621,  0.0288,  ..., -0.0190,  0.1693,  0.0865],\n",
      "        ...,\n",
      "        [-0.0014,  0.1359, -0.0868,  ...,  0.0093, -0.0762, -0.0965],\n",
      "        [-0.1015, -0.0373, -0.0550,  ..., -0.1668, -0.0397, -0.1644],\n",
      "        [-0.0972, -0.0367,  0.0956,  ..., -0.0393, -0.1070,  0.1614]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1405, -0.0822,  0.0731,  0.0365, -0.0810,  0.0289,  0.0462, -0.1240,\n",
      "         0.1139,  0.0327,  0.0557,  0.0356,  0.1032,  0.0333, -0.0348,  0.0845,\n",
      "        -0.1423, -0.1318, -0.1369,  0.1223,  0.0946, -0.1745,  0.0098,  0.0784,\n",
      "        -0.0420, -0.1407,  0.1100, -0.0178, -0.1430,  0.0877,  0.1444,  0.1220],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0518, -0.1696, -0.0987,  0.0972, -0.0430,  0.0135, -0.1163,  0.1016,\n",
      "         0.1465,  0.1679,  0.0049, -0.0606,  0.1072,  0.1397, -0.0945, -0.1069,\n",
      "         0.1389,  0.0616,  0.1865, -0.0012, -0.0935, -0.1640, -0.0700,  0.0458,\n",
      "         0.0403, -0.1515,  0.0248, -0.0479, -0.1928, -0.0458,  0.1578, -0.0507],\n",
      "       device='cuda:0', requires_grad=True)]]\n"
     ]
    }
   ],
   "source": [
    "print(rnn.rnn.all_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
